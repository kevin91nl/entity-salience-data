{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The goal of this notebook is to demonstrate the preprocessing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "sys.path.append(os.path.join('..', '..'))\n",
    "\n",
    "from utilities.dataset import load_dataset\n",
    "from utilities.dataset import generate_embeddings, Embeddings, Tokenizer, TextEncoder\n",
    "from utilities.dataset import LowercaseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4\n",
      "hello  0.141124  0.032013  0.078299  0.179271  0.149405\n",
      "world -0.078182  0.076007 -0.012109 -0.008258  0.032848\n"
     ]
    }
   ],
   "source": [
    "# First, generate word embeddings with an embedding size of 5\n",
    "word_embeddings = generate_embeddings(['hello', 'world'], embedding_size=5)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:              {'__PAD__': 0, '__UNK__': 1, '__START__': 2, '__END__': 3, 'hello': 4, 'world': 5}\n",
      "Inverse vocabulary:      {0: '__PAD__', 1: '__UNK__', 2: '__START__', 3: '__END__', 4: 'hello', 5: 'world'}\n",
      "Lookup token \"world\":    5\n",
      "Inverse lookup for \"5\":  world\n",
      "Get the weights:\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.14112419  0.14112419  0.14112419  0.14112419  0.14112419]\n",
      " [ 0.03201258  0.03201258  0.03201258  0.03201258  0.03201258]\n",
      " [ 0.07829904  0.07829904  0.07829904  0.07829904  0.07829904]\n",
      " [ 0.14112419  0.03201258  0.07829904  0.17927146  0.14940464]\n",
      " [-0.07818223  0.07600707 -0.01210858 -0.00825751  0.03284788]]\n"
     ]
    }
   ],
   "source": [
    "# Then, augment the embeddings with special markers\n",
    "# See the documentation for an overview of all available markers\n",
    "enriched_word_embeddings = Embeddings(word_embeddings)\n",
    "print('Vocabulary:             ', enriched_word_embeddings.get_vocab())\n",
    "print('Inverse vocabulary:     ', enriched_word_embeddings.get_inverse_vocab())\n",
    "print('Lookup token \"world\":   ', enriched_word_embeddings.lookup(\"world\"))\n",
    "print('Inverse lookup for \"5\": ', enriched_word_embeddings.inverse_lookup(5))\n",
    "print('Get the weights:')\n",
    "print(enriched_word_embeddings.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK word tokenize on \"hello world\":\n",
      "   ['hello', 'world']\n",
      "LowercaseTransformer on \"Hello World\":\n",
      "   hello world\n",
      "Result of word_tokenizer(\"Hello World\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['__START__', 'Hello', 'World', '__END__'],\n",
       " 'normalized_tokens': ['__START__', 'hello', 'world', '__END__'],\n",
       " 'ids': [2, 4, 5, 3]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can create a tokenizer using the Embeddings\n",
    "word_tokenizer = Tokenizer(enriched_word_embeddings, nltk.word_tokenize, transformers=[LowercaseTransformer()])\n",
    "# The subtokenizer is the nltk.word_tokenize method, which takes in a text and produces a list of tokens (words):\n",
    "print('NLTK word tokenize on \"hello world\":\\n  ', nltk.word_tokenize('hello world'))\n",
    "# The LowercaseTransformer takes a word and produces the lowercase variant of that word:\n",
    "lowercase_transformer = LowercaseTransformer()\n",
    "print('LowercaseTransformer on \"Hello World\":\\n  ', lowercase_transformer('Hello World'))\n",
    "# The Tokenizer takes a text, tokenizes the text, produces normalized tokens using all the transformers and returns all of these!\n",
    "print('Result of word_tokenizer(\"Hello World\"):')\n",
    "word_tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['__START__', 'H', 'e', 'l', 'l', 'o', '__END__'],\n",
       " 'normalized_tokens': ['__START__', 'h', 'e', 'l', 'l', 'o', '__END__'],\n",
       " 'ids': [2, 11, 8, 15, 15, 18, 3]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same can be done for characters:\n",
    "char_embeddings = Embeddings(generate_embeddings(list('abcdefghijklmnopqrstuvwxyz'), 7))\n",
    "char_tokenizer = Tokenizer(char_embeddings, lambda token: list(token), transformers=[LowercaseTransformer()])\n",
    "char_tokenizer('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity__word_tokens': ['__START__', 'python', '__END__'],\n",
       " 'entity__word_ids': array([2, 1, 3], dtype=int32),\n",
       " 'entity__char_tokens': [['__START__', '__END__'],\n",
       "  ['__START__', 'p', 'y', 't', 'h', 'o', 'n', '__END__'],\n",
       "  ['__START__', '__END__']],\n",
       " 'entity__char_ids': [array([2, 3], dtype=int32),\n",
       "  array([ 2, 19, 28, 23, 11, 18, 17,  3], dtype=int32),\n",
       "  array([2, 3], dtype=int32)],\n",
       " 'document__word_tokens': ['__START__',\n",
       "  'python',\n",
       "  'is',\n",
       "  'cool',\n",
       "  '!',\n",
       "  '__END__'],\n",
       " 'document__word_ids': array([2, 1, 1, 1, 1, 3], dtype=int32),\n",
       " 'document__char_tokens': [['__START__', '__END__'],\n",
       "  ['__START__', 'p', 'y', 't', 'h', 'o', 'n', '__END__'],\n",
       "  ['__START__', 'i', 's', '__END__'],\n",
       "  ['__START__', 'c', 'o', 'o', 'l', '__END__'],\n",
       "  ['__START__', '!', '__END__'],\n",
       "  ['__START__', '__END__']],\n",
       " 'document__char_ids': [array([2, 3], dtype=int32),\n",
       "  array([ 2, 19, 28, 23, 11, 18, 17,  3], dtype=int32),\n",
       "  array([ 2, 12, 22,  3], dtype=int32),\n",
       "  array([ 2,  6, 18, 18, 15,  3], dtype=int32),\n",
       "  array([2, 1, 3], dtype=int32),\n",
       "  array([2, 3], dtype=int32)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both the word tokenizer and character-based tokenizer are used for the WikiPhraseDataset\n",
    "# See how the following code encodes an entity and a document\n",
    "word_embeddings = Embeddings(generate_embeddings(['hello', 'world'], 10))\n",
    "word_tokenizer = Tokenizer(word_embeddings, nltk.word_tokenize, transformers=[LowercaseTransformer()])\n",
    "char_embeddings = Embeddings(generate_embeddings(list('abcdefghijklmnopqrstuvwxyz'), 7))\n",
    "char_tokenizer = Tokenizer(char_embeddings, lambda token: list(token), transformers=[LowercaseTransformer()])\n",
    "text_encoder = TextEncoder(word_tokenizer, char_tokenizer)\n",
    "text_encoder(entity='Python', document='Python is cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
