{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The goal of this notebook is to demonstrate the preprocessing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.conda/envs/env/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "sys.path.append(os.path.join('..', '..'))\n",
    "\n",
    "from utilities.dataset import load_dataset, generate_embeddings, Embeddings, Tokenizer, TextEncoder, LowercaseTransformer, compute_phrase_mask, create_wikiphrase_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4\n",
      "hello  0.141124  0.032013  0.078299  0.179271  0.149405\n",
      "world -0.078182  0.076007 -0.012109 -0.008258  0.032848\n"
     ]
    }
   ],
   "source": [
    "# First, generate word embeddings with an embedding size of 5\n",
    "word_embeddings = generate_embeddings(['hello', 'world'], embedding_size=5)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:              {'__PAD__': 0, '__UNK__': 1, '__START__': 2, '__END__': 3, 'hello': 4, 'world': 5}\n",
      "Inverse vocabulary:      {0: '__PAD__', 1: '__UNK__', 2: '__START__', 3: '__END__', 4: 'hello', 5: 'world'}\n",
      "Lookup token \"world\":    5\n",
      "Inverse lookup for \"5\":  world\n",
      "Get the weights:\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.14112419  0.14112419  0.14112419  0.14112419  0.14112419]\n",
      " [ 0.03201258  0.03201258  0.03201258  0.03201258  0.03201258]\n",
      " [ 0.07829904  0.07829904  0.07829904  0.07829904  0.07829904]\n",
      " [ 0.14112419  0.03201258  0.07829904  0.17927146  0.14940464]\n",
      " [-0.07818223  0.07600707 -0.01210858 -0.00825751  0.03284788]]\n"
     ]
    }
   ],
   "source": [
    "# Then, augment the embeddings with special markers\n",
    "# See the documentation for an overview of all available markers\n",
    "enriched_word_embeddings = Embeddings(word_embeddings)\n",
    "print('Vocabulary:             ', enriched_word_embeddings.get_vocab())\n",
    "print('Inverse vocabulary:     ', enriched_word_embeddings.get_inverse_vocab())\n",
    "print('Lookup token \"world\":   ', enriched_word_embeddings.lookup(\"world\"))\n",
    "print('Inverse lookup for \"5\": ', enriched_word_embeddings.inverse_lookup(5))\n",
    "print('Get the weights:')\n",
    "print(enriched_word_embeddings.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK word tokenize on \"hello world\":\n",
      "   ['hello', 'world']\n",
      "LowercaseTransformer on \"Hello World\":\n",
      "   hello world\n",
      "Result of word_tokenizer(\"Hello World\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['__START__', 'Hello', 'World', '__END__'],\n",
       " 'normalized_tokens': ['__START__', 'hello', 'world', '__END__'],\n",
       " 'ids': [2, 4, 5, 3]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can create a tokenizer using the Embeddings\n",
    "word_tokenizer = Tokenizer(enriched_word_embeddings, nltk.word_tokenize, transformers=[LowercaseTransformer()])\n",
    "# The subtokenizer is the nltk.word_tokenize method, which takes in a text and produces a list of tokens (words):\n",
    "print('NLTK word tokenize on \"hello world\":\\n  ', nltk.word_tokenize('hello world'))\n",
    "# The LowercaseTransformer takes a word and produces the lowercase variant of that word:\n",
    "lowercase_transformer = LowercaseTransformer()\n",
    "print('LowercaseTransformer on \"Hello World\":\\n  ', lowercase_transformer('Hello World'))\n",
    "# The Tokenizer takes a text, tokenizes the text, produces normalized tokens using all the transformers and returns all of these!\n",
    "print('Result of word_tokenizer(\"Hello World\"):')\n",
    "word_tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['__START__', 'H', 'e', 'l', 'l', 'o', '__END__'],\n",
       " 'normalized_tokens': ['__START__', 'h', 'e', 'l', 'l', 'o', '__END__'],\n",
       " 'ids': [2, 11, 8, 15, 15, 18, 3]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same can be done for characters:\n",
    "char_embeddings = Embeddings(generate_embeddings(list('abcdefghijklmnopqrstuvwxyz'), 7))\n",
    "char_tokenizer = Tokenizer(char_embeddings, lambda token: list(token), transformers=[LowercaseTransformer()])\n",
    "char_tokenizer('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity__word_tokens': ['__START__', 'python', '__END__'],\n",
       " 'entity__word_ids': array([2, 1, 3], dtype=int32),\n",
       " 'entity__char_tokens': [['__START__', '__END__'],\n",
       "  ['__START__', 'p', 'y', 't', 'h', 'o', 'n', '__END__'],\n",
       "  ['__START__', '__END__']],\n",
       " 'entity__char_ids': [array([2, 3], dtype=int32),\n",
       "  array([ 2, 19, 28, 23, 11, 18, 17,  3], dtype=int32),\n",
       "  array([2, 3], dtype=int32)],\n",
       " 'document__word_tokens': ['__START__',\n",
       "  'python',\n",
       "  'is',\n",
       "  'cool',\n",
       "  '!',\n",
       "  '__END__'],\n",
       " 'document__word_ids': array([2, 1, 1, 1, 1, 3], dtype=int32),\n",
       " 'document__char_tokens': [['__START__', '__END__'],\n",
       "  ['__START__', 'p', 'y', 't', 'h', 'o', 'n', '__END__'],\n",
       "  ['__START__', 'i', 's', '__END__'],\n",
       "  ['__START__', 'c', 'o', 'o', 'l', '__END__'],\n",
       "  ['__START__', '!', '__END__'],\n",
       "  ['__START__', '__END__']],\n",
       " 'document__char_ids': [array([2, 3], dtype=int32),\n",
       "  array([ 2, 19, 28, 23, 11, 18, 17,  3], dtype=int32),\n",
       "  array([ 2, 12, 22,  3], dtype=int32),\n",
       "  array([ 2,  6, 18, 18, 15,  3], dtype=int32),\n",
       "  array([2, 1, 3], dtype=int32),\n",
       "  array([2, 3], dtype=int32)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both the word tokenizer and character-based tokenizer are used for the WikiPhraseDataset\n",
    "# See how the following code encodes an entity and a document\n",
    "word_embeddings = Embeddings(generate_embeddings(['hello', 'world'], 10))\n",
    "word_tokenizer = Tokenizer(word_embeddings, nltk.word_tokenize, transformers=[LowercaseTransformer()])\n",
    "char_embeddings = Embeddings(generate_embeddings(list('abcdefghijklmnopqrstuvwxyz'), 7))\n",
    "char_tokenizer = Tokenizer(char_embeddings, lambda token: list(token), transformers=[LowercaseTransformer()])\n",
    "text_encoder = TextEncoder(word_tokenizer, char_tokenizer)\n",
    "text_encoder(entity='Python', document='Python is cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now compute a phrase mask, that is all start words of phrases are masked as 2, all non start words of phrases are masked as 1 and all words not belonging to a phrase are marked as 0\n",
    "compute_phrase_mask('Hello world, this is a test! This is the second sentence. This is another sentence. And this is a test!', ['this is a test', 'another sentence'], word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../utilities/dataset.py:172: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_selected = df_selected[df.annotator == annotator]\n",
      "../../utilities/dataset.py:174: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_selected = df_selected[df.entity == entity]\n"
     ]
    }
   ],
   "source": [
    "# Now load the phrases and create the WikiPhrase dataset\n",
    "df_phrases = load_dataset(os.path.join('..', '..', 'data', 'wikiphrase'))\n",
    "dataset = create_wikiphrase_dataset(df_phrases, text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator</th>\n",
       "      <th>entity</th>\n",
       "      <th>entity__char_ids</th>\n",
       "      <th>entity__char_tokens</th>\n",
       "      <th>entity__word_ids</th>\n",
       "      <th>entity__word_tokens</th>\n",
       "      <th>kb</th>\n",
       "      <th>kb__char_ids</th>\n",
       "      <th>kb__char_tokens</th>\n",
       "      <th>kb__word_ids</th>\n",
       "      <th>kb__word_tokens</th>\n",
       "      <th>phrase_mask</th>\n",
       "      <th>salience</th>\n",
       "      <th>text</th>\n",
       "      <th>text__char_ids</th>\n",
       "      <th>text__char_tokens</th>\n",
       "      <th>text__word_ids</th>\n",
       "      <th>text__word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>kevin</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>[[2, 3], [2, 4, 9, 10, 11, 4, 17, 12, 22, 23, ...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, f, g, h,...</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>[__START__, afghanistan, __END__]</td>\n",
       "      <td>Afghanistan ( (listen); Pashto/Dari: افغانستان...</td>\n",
       "      <td>[[2, 3], [2, 4, 9, 10, 11, 4, 17, 12, 22, 23, ...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, f, g, h,...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, afghanistan, (, (, listen, ), ;, p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>A \"critical\" campaign ad, launched by the US R...</td>\n",
       "      <td>[[2, 3], [2, 4, 3], [2, 1, 1, 3], [2, 6, 21, 1...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, __END__]...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, a, ``, critical, '', campaign, ad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>kevin</td>\n",
       "      <td>AOL</td>\n",
       "      <td>[[2, 3], [2, 4, 18, 15, 3], [2, 3]]</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, o, l, __...</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>[__START__, aol, __END__]</td>\n",
       "      <td>AOL (stylized as Aol., formerly a company know...</td>\n",
       "      <td>[[2, 3], [2, 4, 18, 15, 3], [2, 1, 3], [2, 22,...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, o, l, __...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, aol, (, stylized, as, aol., ,, for...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Internet rivals Microsoft and Yahoo have agree...</td>\n",
       "      <td>[[2, 3], [2, 12, 17, 23, 8, 21, 17, 8, 23, 3],...</td>\n",
       "      <td>[[__START__, __END__], [__START__, i, n, t, e,...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, internet, rivals, microsoft, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>kevin</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>[[2, 3], [2, 4, 23, 15, 4, 17, 23, 4, 3], [2, 3]]</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, t, l, a,...</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>[__START__, atlanta, __END__]</td>\n",
       "      <td>Atlanta () is the capital of, and the most pop...</td>\n",
       "      <td>[[2, 3], [2, 4, 23, 15, 4, 17, 23, 4, 3], [2, ...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, t, l, a,...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, atlanta, (, ), is, the, capital, o...</td>\n",
       "      <td>[0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>A man infected with extensively drug resistant...</td>\n",
       "      <td>[[2, 3], [2, 4, 3], [2, 16, 4, 17, 3], [2, 12,...</td>\n",
       "      <td>[[__START__, __END__], [__START__, a, __END__]...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[__START__, a, man, infected, with, extensivel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotator       entity                                   entity__char_ids  \\\n",
       "63     kevin  Afghanistan  [[2, 3], [2, 4, 9, 10, 11, 4, 17, 12, 22, 23, ...   \n",
       "34     kevin          AOL                [[2, 3], [2, 4, 18, 15, 3], [2, 3]]   \n",
       "77     kevin      Atlanta  [[2, 3], [2, 4, 23, 15, 4, 17, 23, 4, 3], [2, 3]]   \n",
       "\n",
       "                                  entity__char_tokens entity__word_ids  \\\n",
       "63  [[__START__, __END__], [__START__, a, f, g, h,...        [2, 1, 3]   \n",
       "34  [[__START__, __END__], [__START__, a, o, l, __...        [2, 1, 3]   \n",
       "77  [[__START__, __END__], [__START__, a, t, l, a,...        [2, 1, 3]   \n",
       "\n",
       "                  entity__word_tokens  \\\n",
       "63  [__START__, afghanistan, __END__]   \n",
       "34          [__START__, aol, __END__]   \n",
       "77      [__START__, atlanta, __END__]   \n",
       "\n",
       "                                                   kb  \\\n",
       "63  Afghanistan ( (listen); Pashto/Dari: افغانستان...   \n",
       "34  AOL (stylized as Aol., formerly a company know...   \n",
       "77  Atlanta () is the capital of, and the most pop...   \n",
       "\n",
       "                                         kb__char_ids  \\\n",
       "63  [[2, 3], [2, 4, 9, 10, 11, 4, 17, 12, 22, 23, ...   \n",
       "34  [[2, 3], [2, 4, 18, 15, 3], [2, 1, 3], [2, 22,...   \n",
       "77  [[2, 3], [2, 4, 23, 15, 4, 17, 23, 4, 3], [2, ...   \n",
       "\n",
       "                                      kb__char_tokens  \\\n",
       "63  [[__START__, __END__], [__START__, a, f, g, h,...   \n",
       "34  [[__START__, __END__], [__START__, a, o, l, __...   \n",
       "77  [[__START__, __END__], [__START__, a, t, l, a,...   \n",
       "\n",
       "                                         kb__word_ids  \\\n",
       "63  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "34  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "77  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      kb__word_tokens  \\\n",
       "63  [__START__, afghanistan, (, (, listen, ), ;, p...   \n",
       "34  [__START__, aol, (, stylized, as, aol., ,, for...   \n",
       "77  [__START__, atlanta, (, ), is, the, capital, o...   \n",
       "\n",
       "                                          phrase_mask  salience  \\\n",
       "63  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.333333   \n",
       "34  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.500000   \n",
       "77  [0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  0.666667   \n",
       "\n",
       "                                                 text  \\\n",
       "63  A \"critical\" campaign ad, launched by the US R...   \n",
       "34  Internet rivals Microsoft and Yahoo have agree...   \n",
       "77  A man infected with extensively drug resistant...   \n",
       "\n",
       "                                       text__char_ids  \\\n",
       "63  [[2, 3], [2, 4, 3], [2, 1, 1, 3], [2, 6, 21, 1...   \n",
       "34  [[2, 3], [2, 12, 17, 23, 8, 21, 17, 8, 23, 3],...   \n",
       "77  [[2, 3], [2, 4, 3], [2, 16, 4, 17, 3], [2, 12,...   \n",
       "\n",
       "                                    text__char_tokens  \\\n",
       "63  [[__START__, __END__], [__START__, a, __END__]...   \n",
       "34  [[__START__, __END__], [__START__, i, n, t, e,...   \n",
       "77  [[__START__, __END__], [__START__, a, __END__]...   \n",
       "\n",
       "                                       text__word_ids  \\\n",
       "63  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "34  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "77  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                    text__word_tokens  \n",
       "63  [__START__, a, ``, critical, '', campaign, ad,...  \n",
       "34  [__START__, internet, rivals, microsoft, and, ...  \n",
       "77  [__START__, a, man, infected, with, extensivel...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that the dataset is just a Dataframe\n",
    "dataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity word tokens:\n",
      "['__START__', 'digital', 'camera', '__END__']\n",
      "\n",
      "Knowledge base word tokens:\n",
      "['__START__', 'a', 'digital', 'camera', 'or', 'digicam', 'is', 'a', 'camera', 'that', 'captures', 'photographs', 'in', 'digital', 'memory', '.', 'most', 'cameras', 'produced', 'today', 'are', 'digital', ',', 'and', 'while', 'there', 'are', 'still', 'dedicated', 'digital', 'cameras', ',', 'many', 'more', 'are', 'now', 'incorporated', 'into', 'devices', 'ranging', 'from', 'mobile', 'devices', 'to', 'vehicles', '.', '__END__']\n",
      "\n",
      "Text tokens with phrase mask:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '__START__'),\n",
       " (0, 'south'),\n",
       " (0, 'korean'),\n",
       " (0, 'electronics'),\n",
       " (0, 'giant'),\n",
       " (0, 'samsung'),\n",
       " (0, 'has'),\n",
       " (0, 'begun'),\n",
       " (0, 'mass'),\n",
       " (0, 'production'),\n",
       " (0, 'of'),\n",
       " (0, 'a'),\n",
       " (0, 'new'),\n",
       " (0, '4'),\n",
       " (0, 'gigabit'),\n",
       " (0, 'flash'),\n",
       " (0, 'memory'),\n",
       " (0, '.'),\n",
       " (0, 'this'),\n",
       " (0, 'is'),\n",
       " (0, 'twice'),\n",
       " (0, 'the'),\n",
       " (0, 'size'),\n",
       " (0, 'of'),\n",
       " (0, 'almost'),\n",
       " (0, 'all'),\n",
       " (0, 'the'),\n",
       " (0, 'biggest'),\n",
       " (0, 'solid'),\n",
       " (0, 'state'),\n",
       " (0, 'memory'),\n",
       " (0, 'devices'),\n",
       " (0, 'currently'),\n",
       " (0, 'available'),\n",
       " (0, '.'),\n",
       " (0, 'the'),\n",
       " (0, 'firm'),\n",
       " (0, 'is'),\n",
       " (0, 'using'),\n",
       " (0, 'a'),\n",
       " (0, 'new'),\n",
       " (0, '70-nanometer'),\n",
       " (0, 'process'),\n",
       " (0, 'which'),\n",
       " (0, 'enables'),\n",
       " (0, 'it'),\n",
       " (0, 'to'),\n",
       " (0, 'produce'),\n",
       " (0, 'the'),\n",
       " (0, 'smallest'),\n",
       " (0, 'cell'),\n",
       " (0, 'on'),\n",
       " (0, 'the'),\n",
       " (0, 'market'),\n",
       " (0, ','),\n",
       " (0, 'enabling'),\n",
       " (0, 'a'),\n",
       " (0, 'far'),\n",
       " (0, 'higher'),\n",
       " (0, 'density'),\n",
       " (0, 'than'),\n",
       " (0, 'competitors'),\n",
       " (0, '.'),\n",
       " (0, 'the'),\n",
       " (0, 'device'),\n",
       " (0, 'can'),\n",
       " (0, 'write'),\n",
       " (0, 'data'),\n",
       " (0, 'at'),\n",
       " (0, '16-megabytes'),\n",
       " (0, 'per'),\n",
       " (0, 'second'),\n",
       " (0, ','),\n",
       " (0, 'twice'),\n",
       " (0, 'as'),\n",
       " (0, 'fast'),\n",
       " (0, 'as'),\n",
       " (0, 'a'),\n",
       " (0, '90nm'),\n",
       " (0, '2gb'),\n",
       " (0, 'device'),\n",
       " (0, '.'),\n",
       " (2, 'possible'),\n",
       " (1, 'applications'),\n",
       " (1, 'of'),\n",
       " (1, 'the'),\n",
       " (1, 'new'),\n",
       " (1, 'memory'),\n",
       " (1, 'will'),\n",
       " (1, 'most'),\n",
       " (1, 'likely'),\n",
       " (1, 'be'),\n",
       " (1, 'as'),\n",
       " (1, 'a'),\n",
       " (1, 'component'),\n",
       " (1, 'in'),\n",
       " (1, 'mp3'),\n",
       " (1, 'players'),\n",
       " (1, ','),\n",
       " (1, 'top'),\n",
       " (1, 'end'),\n",
       " (1, 'mobile'),\n",
       " (1, 'phones'),\n",
       " (1, ','),\n",
       " (1, 'usb'),\n",
       " (1, 'memory'),\n",
       " (1, 'sticks'),\n",
       " (1, ','),\n",
       " (1, 'digital'),\n",
       " (1, 'cameras'),\n",
       " (1, ','),\n",
       " (1, 'and'),\n",
       " (1, 'other'),\n",
       " (1, 'portable'),\n",
       " (1, 'devices'),\n",
       " (0, '.'),\n",
       " (0, 'the'),\n",
       " (0, 'new'),\n",
       " (0, 'form'),\n",
       " (0, 'of'),\n",
       " (0, 'eeprom'),\n",
       " (0, 'memory'),\n",
       " (0, 'writes'),\n",
       " (0, 'and'),\n",
       " (0, 'reads'),\n",
       " (0, 'data'),\n",
       " (0, 'faster'),\n",
       " (0, 'than'),\n",
       " (0, 'previous'),\n",
       " (0, 'versions'),\n",
       " (0, '&'),\n",
       " (0, 'mdash'),\n",
       " (0, ';'),\n",
       " (0, 'up'),\n",
       " (0, 'to'),\n",
       " (0, '16'),\n",
       " (0, 'megabytes'),\n",
       " (0, 'per'),\n",
       " (0, 'second'),\n",
       " (0, '.'),\n",
       " (0, 'the'),\n",
       " (0, 'storage'),\n",
       " (0, 'of'),\n",
       " (0, 'real-time'),\n",
       " (0, 'high-definition'),\n",
       " (0, 'video'),\n",
       " (0, 'is'),\n",
       " (0, 'feasible'),\n",
       " (0, 'on'),\n",
       " (0, 'the'),\n",
       " (0, 'chips'),\n",
       " (0, ','),\n",
       " (0, 'according'),\n",
       " (0, 'to'),\n",
       " (0, 'samsung'),\n",
       " (0, '.'),\n",
       " (0, 'the'),\n",
       " (0, 'company'),\n",
       " (0, 'also'),\n",
       " (0, 'believes'),\n",
       " (0, 'their'),\n",
       " (0, 'flash-based'),\n",
       " (0, 'disk'),\n",
       " (0, 'drives'),\n",
       " (0, 'could'),\n",
       " (0, 'replace'),\n",
       " (0, 'hard'),\n",
       " (0, 'drives'),\n",
       " (0, 'in'),\n",
       " (0, 'laptops'),\n",
       " (0, 'since'),\n",
       " (0, 'they'),\n",
       " (0, 'will'),\n",
       " (0, 'work'),\n",
       " (0, 'faster'),\n",
       " (0, ','),\n",
       " (0, 'quieter'),\n",
       " (0, 'and'),\n",
       " (0, 'cooler'),\n",
       " (0, 'than'),\n",
       " (0, 'drives'),\n",
       " (0, 'currently'),\n",
       " (0, 'in'),\n",
       " (0, 'use'),\n",
       " (0, '.'),\n",
       " (0, '__END__')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load an example\n",
    "example = dataset.sample(1).iloc[0]\n",
    "\n",
    "# Display the entity word tokens\n",
    "print('Entity word tokens:')\n",
    "print(example['entity__word_tokens'])\n",
    "print()\n",
    "\n",
    "# Display the knowledge base word tokens\n",
    "print('Knowledge base word tokens:')\n",
    "print(example['kb__word_tokens'])\n",
    "print()\n",
    "\n",
    "# Display a combination of the phrase mask and the text tokens\n",
    "print('Text tokens with phrase mask:')\n",
    "list(zip(example['phrase_mask'], example['text__word_tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
