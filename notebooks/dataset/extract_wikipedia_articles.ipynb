{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Wikipedia articles\n",
    "\n",
    "This notebook is intended for extracting Wikipedia articles for the entities which are available in Wikinews and storing them in the `data/wikiphrase` folder in the `wikipedia-entities.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "sys.path.append(os.path.join('..', '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', '..', 'data', 'wikiphrase')\n",
    "df_entities = pd.read_json(os.path.join(data_path, 'wikinews-entities.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia fetching function\n",
    "\n",
    "There are two types of main errors while fetching information from Wikipedia. One of the errors is the PageError. This occurs when an article can not be found for the given entity. The other error occurs when multiple Wikipedia pages are related to the given entity. This disambiguation error is solved by picking the first option and by specifying that there was an ambiguation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_article(entity, is_ambiguous=False):\n",
    "    # Try to fetch the Wikipedia article (and a short summary of it)\n",
    "    try:\n",
    "        wikipedia_short_summary = wikipedia.summary(entity, sentences=2)\n",
    "        wikipedia_summary = wikipedia.summary(entity)\n",
    "        return wikipedia_short_summary, wikipedia_summary, is_ambiguous\n",
    "    except PageError:\n",
    "        # Raise a PageError when the article for the entity does not exists\n",
    "        raise\n",
    "    except DisambiguationError as error:\n",
    "        # Go for the first option\n",
    "        return fetch_wikipedia_article(error.options[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia fetch and store\n",
    "\n",
    "The next section fetches the data from Wikipedia and stores it in the `data/wikiphrase/wikipedia-entities.json` file. For the Wikiphrase entities, this took roughly ~30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing New York...:   4%|▍         | 89/2314 [01:33<41:00,  1.11s/it]/opt/conda/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/conda/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Processing Kathleen Kane...: 100%|██████████| 2314/2314 [38:35<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "wikipedia_entries = []\n",
    "\n",
    "entities = tqdm(df_entities.entity.unique())\n",
    "\n",
    "# Loop through all unique entities\n",
    "for entity in entities:\n",
    "    # Update the progressbar\n",
    "    entities.set_description('Processing {}...'.format(entity))\n",
    "    \n",
    "    # Try to fetch the Wikipedia article\n",
    "    try:\n",
    "        wikipedia_short_summary, wikipedia_summary, is_ambiguous = fetch_wikipedia_article(entity)\n",
    "    except PageError:\n",
    "        continue\n",
    "    \n",
    "    # When successful, add it to the list of entries\n",
    "    wikipedia_entries.append({\n",
    "        'entity': entity,\n",
    "        'wikipedia_text_short': wikipedia_short_summary,\n",
    "        'wikipedia_text': wikipedia_summary,\n",
    "        'is_ambiguous': is_ambiguous,\n",
    "        'added_at': datetime.datetime.now()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe out of it\n",
    "df_wiki = pd.DataFrame(wikipedia_entries)\n",
    "df_wiki.added_at = pd.to_datetime(df_wiki.added_at)\n",
    "df_wiki.to_json(os.path.join(data_path, 'wikipedia-entities.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook creates the `wikipedia-entities.json` file which contains Wikipedia information for unambiguous entities found in the Wikiphrase dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
